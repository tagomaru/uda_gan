{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"run_udagan_shm.ipynb","provenance":[{"file_id":"1kfWywYqjOawjPRbvaRaPf6kF-8kh7zup","timestamp":1613941038433},{"file_id":"1Mrd7EdS2TyrrSRfgafMPTfvKYcXkzPho","timestamp":1613890015856}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__ueeuofWzhU","executionInfo":{"status":"ok","timestamp":1613983580486,"user_tz":480,"elapsed":855,"user":{"displayName":"JX L","photoUrl":"","userId":"03475133506377629126"}},"outputId":"d2c22bef-7d5d-4960-b6cb-6dabd80f42bd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pO4DHX1dXLH-","executionInfo":{"status":"ok","timestamp":1613983580486,"user_tz":480,"elapsed":847,"user":{"displayName":"JX L","photoUrl":"","userId":"03475133506377629126"}},"outputId":"76ef1f15-de48-4db8-ddd6-4439cf07ec24"},"source":["cd drive/My\\ Drive/Work\\&Study/stanford/coursework/cs236g/project/uda_gan/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Work&Study/stanford/coursework/cs236g/project/uda_gan\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7_4fZgmUW8A8","executionInfo":{"status":"ok","timestamp":1613983581284,"user_tz":480,"elapsed":1640,"user":{"displayName":"JX L","photoUrl":"","userId":"03475133506377629126"}}},"source":["import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","import time\n","import datetime, itertools\n","from torch.autograd import Function, Variable\n","import torchvision\n","import torch.utils.data as data\n","import torchvision.utils as vutils\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pickle\n","from scipy.signal import butter,lfilter,freqz,detrend,stft\n","from sklearn.utils import shuffle\n","from sklearn.metrics import f1_score\n","from Dann import *\n","from GRL import *\n","from helper_func import *\n","import gan_da_model as model\n","from gan_da_trainer import *\n","from Dataset import *\n","import random\n","# np.random.seed(1000)\n","# torch.backends.cudnn.deterministic = True\n","# torch.backends.cudnn.benchmark = False"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J8WbzrIobBYE"},"source":["# Form dataset "]},{"cell_type":"code","metadata":{"id":"sNohr2uD5XzW","executionInfo":{"status":"ok","timestamp":1613983697461,"user_tz":480,"elapsed":117814,"user":{"displayName":"JX L","photoUrl":"","userId":"03475133506377629126"}}},"source":["# data path\n","filePath = '../../../../../CMU/research/Damage_diagnosis/transfer/dataset/'\n","# Knowledge transfer direction\n","b1b2 = True\n","# Which vehicle\n","v_n =10\n","# Load data\n","x_dann, label_l, label_s, label_d, label_f, _, _=load_feature(filePath,v_n,b1b2)\n","x_dann_test, label_l_test, label_s_test, label_d_test, label_f_test, label_bb, damage_ls=load_feature(filePath,v_n,b1b2)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oaRvd405oSA","executionInfo":{"status":"ok","timestamp":1613983697463,"user_tz":480,"elapsed":117813,"user":{"displayName":"JX L","photoUrl":"","userId":"03475133506377629126"}}},"source":["batch_size = 16\n","# source domain data and labels\n","x_source,yl_source,ys_source,yd_source,yf_source=shuffle(x_dann[0:round(len(x_dann)/2)],label_l,label_s,label_d,label_f,random_state=0)\n","# target domain data and labels\n","x_target,yl_target,ys_target,yd_target,yf_target=shuffle(x_dann[round(len(x_dann)/2):],label_l,label_s,label_d,label_f,random_state=0)\n","# hold out validation set\n","hold_out_size = int(len(x_source)*0.2)\n","\n","dataset_train_source = Dataset(x_source[:-hold_out_size],yl_source[:-hold_out_size],ys_source[:-hold_out_size],\\\n","                             yd_source[:-hold_out_size], yf_source[:-hold_out_size], is_train = True)\n","dataset_test_source = Dataset(x_source[-hold_out_size:],yl_source[-hold_out_size:],ys_source[-hold_out_size:],\\\n","                             yd_source[-hold_out_size:], yf_source[-hold_out_size:], is_train = False)\n","dataset_train_target = Dataset(x_target[:-hold_out_size],yl_target[:-hold_out_size],ys_target[:-hold_out_size],\\\n","                             yd_target[:-hold_out_size], yf_target[:-hold_out_size], is_train = True)\n","dataset_test_target = Dataset(x_target[-hold_out_size:],yl_target[-hold_out_size:],ys_target[-hold_out_size:],\\\n","                             yd_target[-hold_out_size:], yf_target[-hold_out_size:], is_train = False)\n","\n","source_train = torch.utils.data.DataLoader(dataset_train_source,batch_size, shuffle=True)\n","source_train_ = torch.utils.data.DataLoader(dataset_train_source,batch_size, shuffle=True)\n","source_test = torch.utils.data.DataLoader(dataset_test_source,batch_size, shuffle=False)\n","target_train = torch.utils.data.DataLoader(dataset_train_target,batch_size, shuffle=True)\n","target_train_ = torch.utils.data.DataLoader(dataset_train_target,batch_size, shuffle=True)\n","target_test = torch.utils.data.DataLoader(dataset_test_target,batch_size, shuffle=False)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYPjCU5vOHRx"},"source":["# Generate to adapt:\n","https://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Generate_to_Adapt_CVPR_2018_paper.pdf"]},{"cell_type":"markdown","metadata":{"id":"8CcO-UFLq3ZK"},"source":["## Train"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_vu_-x0mR8d","outputId":"a0575017-2e16-4baf-b44e-e9ad3c30acfd"},"source":["class Args:\n","  batchSize=16\n","  nz = 128 #size of the latent z vector\n","  ngf = 64 #Number of filters to use in the generator network\n","  ndf = 64 #Number of filters to use in the discriminator network\n","  nepochs = 3000 #number of epochs to train for\n","  lr = 0.0005 #learning rate, default=0.0002\n","  beta1 = 0.8 #beta1 for adam. default=0.5\n","  gpu = 1 #GPU to use, -1 for CPU training\n","  outf = 'results' #folder to output images and model checkpoints\n","  manualSeed = 1\n","  adv_weight = 0.1 #weight for adv loss\n","  lrd = 0.0001 #learning rate decay, default=0.0002\n","  alpha = 0.3 #multiplicative factor for target adv. loss\n","\n","opt = Args()\n","# print(opt)\n","\n","# Setting random seed\n","if opt.manualSeed is None:\n","    opt.manualSeed = random.randint(1, 10000)\n","print(\"Random Seed: \", opt.manualSeed)\n","random.seed(opt.manualSeed)\n","torch.manual_seed(opt.manualSeed)\n","if opt.gpu>=0:\n","    torch.cuda.manual_seed_all(opt.manualSeed)\n","\n","# GPU/CPU flags\n","torch.backends.cudnn.benchmark = True\n","if torch.cuda.is_available() and opt.gpu == -1:\n","    print(\"WARNING: You have a CUDA device, so you should probably run with --gpu [gpu id]\")\n","if opt.gpu>=0:\n","    os.environ['CUDA_VISIBLE_DEVICES'] = str(opt.gpu)\n","\n","nclasses = 13\n","\n","GTA_trainer = GTA(opt, nclasses, source_train, source_test, target_train)\n","GTA_trainer.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Random Seed:  1\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/My Drive/Work&Study/stanford/coursework/cs236g/project/uda_gan/gan_da_trainer.py:72: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  inputv, labelv = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda())\n"],"name":"stderr"},{"output_type":"stream","text":["2021-02-22 08:49:08.657863| Epoch: 50, Val Accuracy: 70.833333 %\n","Test Accuracy: 19.791667 %\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/My Drive/Work&Study/stanford/coursework/cs236g/project/uda_gan/gan_da_trainer.py:97: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  inputv, labelv = Variable(inputs, volatile=True), Variable(labels)\n"],"name":"stderr"},{"output_type":"stream","text":["2021-02-22 08:49:57.055486| Epoch: 100, Val Accuracy: 87.500000 %\n","Test Accuracy: 27.083333 %\n","2021-02-22 08:50:44.102723| Epoch: 150, Val Accuracy: 85.416667 %\n","Test Accuracy: 28.333333 %\n","2021-02-22 08:51:31.396221| Epoch: 200, Val Accuracy: 82.291667 %\n","Test Accuracy: 27.083333 %\n","2021-02-22 08:52:19.439038| Epoch: 250, Val Accuracy: 86.458333 %\n","Test Accuracy: 23.125000 %\n","2021-02-22 08:53:07.600241| Epoch: 300, Val Accuracy: 85.416667 %\n","Test Accuracy: 27.291667 %\n","2021-02-22 08:53:55.397815| Epoch: 350, Val Accuracy: 88.541667 %\n","Test Accuracy: 24.166667 %\n","2021-02-22 08:54:43.055434| Epoch: 400, Val Accuracy: 87.500000 %\n","Test Accuracy: 30.208333 %\n","2021-02-22 08:55:30.139757| Epoch: 450, Val Accuracy: 84.375000 %\n","Test Accuracy: 25.833333 %\n","2021-02-22 08:56:17.554348| Epoch: 500, Val Accuracy: 84.375000 %\n","Test Accuracy: 28.958333 %\n","2021-02-22 08:57:04.639459| Epoch: 550, Val Accuracy: 83.333333 %\n","Test Accuracy: 28.333333 %\n","2021-02-22 08:57:52.209922| Epoch: 600, Val Accuracy: 87.500000 %\n","Test Accuracy: 26.458333 %\n","2021-02-22 08:58:39.711299| Epoch: 650, Val Accuracy: 86.458333 %\n","Test Accuracy: 30.833333 %\n","2021-02-22 08:59:26.980955| Epoch: 700, Val Accuracy: 81.250000 %\n","Test Accuracy: 30.000000 %\n","2021-02-22 09:00:14.585712| Epoch: 750, Val Accuracy: 85.416667 %\n","Test Accuracy: 30.000000 %\n","2021-02-22 09:01:02.626447| Epoch: 800, Val Accuracy: 83.333333 %\n","Test Accuracy: 30.208333 %\n","2021-02-22 09:01:50.563080| Epoch: 850, Val Accuracy: 84.375000 %\n","Test Accuracy: 30.208333 %\n","2021-02-22 09:02:37.667375| Epoch: 900, Val Accuracy: 84.375000 %\n","Test Accuracy: 28.750000 %\n","2021-02-22 09:03:25.484396| Epoch: 950, Val Accuracy: 86.458333 %\n","Test Accuracy: 28.541667 %\n","2021-02-22 09:04:14.042401| Epoch: 1000, Val Accuracy: 85.416667 %\n","Test Accuracy: 27.500000 %\n","2021-02-22 09:05:01.247335| Epoch: 1050, Val Accuracy: 85.416667 %\n","Test Accuracy: 25.625000 %\n","2021-02-22 09:05:49.407453| Epoch: 1100, Val Accuracy: 84.375000 %\n","Test Accuracy: 30.208333 %\n","2021-02-22 09:06:38.051463| Epoch: 1150, Val Accuracy: 82.291667 %\n","Test Accuracy: 29.166667 %\n","2021-02-22 09:07:25.992322| Epoch: 1200, Val Accuracy: 86.458333 %\n","Test Accuracy: 27.708333 %\n","2021-02-22 09:08:14.339440| Epoch: 1250, Val Accuracy: 86.458333 %\n","Test Accuracy: 32.083333 %\n","2021-02-22 09:09:02.607814| Epoch: 1300, Val Accuracy: 85.416667 %\n","Test Accuracy: 23.125000 %\n","2021-02-22 09:09:50.744628| Epoch: 1350, Val Accuracy: 81.250000 %\n","Test Accuracy: 29.583333 %\n","2021-02-22 09:10:38.499360| Epoch: 1400, Val Accuracy: 81.250000 %\n","Test Accuracy: 27.083333 %\n","2021-02-22 09:11:26.514702| Epoch: 1450, Val Accuracy: 81.250000 %\n","Test Accuracy: 26.875000 %\n","2021-02-22 09:12:14.577880| Epoch: 1500, Val Accuracy: 81.250000 %\n","Test Accuracy: 27.083333 %\n","2021-02-22 09:13:02.948779| Epoch: 1550, Val Accuracy: 81.250000 %\n","Test Accuracy: 35.208333 %\n","2021-02-22 09:13:50.900501| Epoch: 1600, Val Accuracy: 80.208333 %\n","Test Accuracy: 36.458333 %\n","2021-02-22 09:14:38.696208| Epoch: 1650, Val Accuracy: 82.291667 %\n","Test Accuracy: 36.250000 %\n","2021-02-22 09:15:25.901220| Epoch: 1700, Val Accuracy: 81.250000 %\n","Test Accuracy: 36.250000 %\n","2021-02-22 09:16:13.347987| Epoch: 1750, Val Accuracy: 83.333333 %\n","Test Accuracy: 32.500000 %\n","2021-02-22 09:17:01.045935| Epoch: 1800, Val Accuracy: 83.333333 %\n","Test Accuracy: 35.416667 %\n","2021-02-22 09:17:48.703661| Epoch: 1850, Val Accuracy: 82.291667 %\n","Test Accuracy: 36.458333 %\n","2021-02-22 09:18:36.320966| Epoch: 1900, Val Accuracy: 86.458333 %\n","Test Accuracy: 37.500000 %\n","2021-02-22 09:19:23.776573| Epoch: 1950, Val Accuracy: 87.500000 %\n","Test Accuracy: 32.916667 %\n","2021-02-22 09:20:11.316602| Epoch: 2000, Val Accuracy: 87.500000 %\n","Test Accuracy: 37.291667 %\n","2021-02-22 09:20:58.426425| Epoch: 2050, Val Accuracy: 86.458333 %\n","Test Accuracy: 35.416667 %\n","2021-02-22 09:21:46.159735| Epoch: 2100, Val Accuracy: 86.458333 %\n","Test Accuracy: 34.166667 %\n","2021-02-22 09:22:33.270074| Epoch: 2150, Val Accuracy: 84.375000 %\n","Test Accuracy: 27.916667 %\n","2021-02-22 09:23:20.807730| Epoch: 2200, Val Accuracy: 83.333333 %\n","Test Accuracy: 34.375000 %\n","2021-02-22 09:24:08.578587| Epoch: 2250, Val Accuracy: 85.416667 %\n","Test Accuracy: 32.083333 %\n","2021-02-22 09:24:56.096868| Epoch: 2300, Val Accuracy: 86.458333 %\n","Test Accuracy: 37.083333 %\n","2021-02-22 09:25:43.434767| Epoch: 2350, Val Accuracy: 87.500000 %\n","Test Accuracy: 35.625000 %\n","2021-02-22 09:26:30.828730| Epoch: 2400, Val Accuracy: 85.416667 %\n","Test Accuracy: 32.291667 %\n","2021-02-22 09:27:18.589226| Epoch: 2450, Val Accuracy: 84.375000 %\n","Test Accuracy: 27.083333 %\n","2021-02-22 09:28:05.959395| Epoch: 2500, Val Accuracy: 85.416667 %\n","Test Accuracy: 27.916667 %\n","2021-02-22 09:28:52.969896| Epoch: 2550, Val Accuracy: 85.416667 %\n","Test Accuracy: 33.541667 %\n","2021-02-22 09:29:40.321629| Epoch: 2600, Val Accuracy: 86.458333 %\n","Test Accuracy: 36.250000 %\n","2021-02-22 09:30:27.764739| Epoch: 2650, Val Accuracy: 83.333333 %\n","Test Accuracy: 35.000000 %\n","2021-02-22 09:31:15.128358| Epoch: 2700, Val Accuracy: 84.375000 %\n","Test Accuracy: 33.958333 %\n","2021-02-22 09:32:02.244242| Epoch: 2750, Val Accuracy: 85.416667 %\n","Test Accuracy: 36.666667 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g-3xZDWPq5fI"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"NL8h7sZ7q5MD"},"source":["class Args:\n","  batchSize=100\n","  nz = 128 #size of the latent z vector\n","  ngf = 64 #Number of filters to use in the generator network\n","  ndf = 64 #Number of filters to use in the discriminator network\n","  gpu = 1 #GPU to use, -1 for CPU training\n","  checkpoint_dir = 'results/models'\n","  model_best = 0\n","\n","opt = Args()\n","\n","netF = model._netF(opt)\n","netC = model._netC(opt, nclasses)\n","\n","if opt.model_best == 0: \n","    netF_path = os.path.join(opt.checkpoint_dir, 'netF.pth')\n","    netC_path = os.path.join(opt.checkpoint_dir, 'netC.pth')\n","else:\n","    netF_path = os.path.join(opt.checkpoint_dir, 'model_best_netF.pth')\n","    netC_path = os.path.join(opt.checkpoint_dir, 'model_best_netC.pth')\n","    \n","netF.load_state_dict(torch.load(netF_path))\n","netC.load_state_dict(torch.load(netC_path))\n","\n","if opt.gpu>=0:\n","    netF.cuda()\n","    netC.cuda()\n","    \n","# Testing\n","\n","netF.eval()\n","netC.eval()\n","    \n","total = 0\n","correct = 0\n","\n","for i, datas in enumerate(target_train):\n","    inputs,_,_,_,labels = datas\n","    if opt.gpu>=0:\n","        inputs, labels = inputs.cuda(), labels.cuda()\n","    inputv, labelv = Variable(inputs, volatile=True), Variable(labels)\n","\n","    outC = netC(netF(inputv))\n","    _, predicted = torch.max(outC.data, 1)        \n","    total += labels.size(0)\n","    correct += ((predicted == labels.cuda()).sum())\n","    \n","test_acc = 100*float(correct)/total\n","print('Test Accuracy: %f %%' % (test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_mXXrLs_OA_E"},"source":["# DANN approach (not a GAN baseline)\n","https://arxiv.org/abs/2006.03641"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BK5yAEIs6F9","outputId":"c7b120b9-0be4-4be7-942f-a0d20da39d77"},"source":["device='cuda'\n","model = Dann().to(device)\n","optimizer = optim.SGD(model.parameters(), lr= 0.001, momentum= 0.9)\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","allepoch=500\n","CUDA_LAUNCH_BLOCKING=1\n","losses = []\n","accuracies = []\n","f1s = []\n","losses_save,accuracy_save,f1_save = test_epoch(0,100,source_train,source_test,\\\n","                                               target_train,target_test,model,criterion)\n","losses = np.append(losses,losses_save)\n","accuracies=np.append(accuracies,accuracy_save)\n","f1s=np.append(f1s,f1_save)\n","for epoch in range(allepoch):\n","    len_dataloader = min(len(source_train), len(target_train))\n","    total_steps = allepoch * len(source_train)\n","    i = 0\n","    model.train()\n","    for batch_idx, (data_source, data_source_, \\\n","                    data_target, data_target_) in enumerate(zip(source_train, \\\n","                    source_train_, target_train, target_train_)):\n","        start_time = time.time()\n","        s_img, s_label_l, s_label_s,_,_ = data_source\n","        s_img_, s_label_l_, s_label_s_,_,_ = data_source_\n","\n","        start_steps = epoch * len(source_train)\n","\n","        p = float(i + start_steps) / total_steps\n","        diff = 1.\n","        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n","\n","        optimizer = optimizer_scheduler(optimizer, p)\n","        optimizer.zero_grad()\n","\n","        batch_size = len(s_label_l)\n","\n","        domain_label_s = torch.zeros(2*batch_size)\n","        domain_label_s = domain_label_s.long()\n","\n","        # source domain\n","        input_s = (s_img.to(device),s_img_.to(device))\n","        a,b,c,d = model(input_s,alpha)\n","        latent_xl,latent_xq = d\n","        pred_dl, pred_dq = b\n","        err_s_label = criterion(a, s_label_l.to(device))\n","        err_s_label_s = criterion(c, s_label_s_.to(device))\n","        err_s_domain = criterion(torch.cat((pred_dl,pred_dq)), domain_label_s.to(device))\n","\n","        # training model using target data\n","        t_img, t_label_l, t_label_s,_,_ = data_target\n","        t_img_, t_label_l_, t_label_s_,_,_ = data_target_\n","        domain_label_t = torch.ones(2*batch_size)\n","        domain_label_t = domain_label_t.long()\n","        input_t = (t_img.to(device),t_img_.to(device))\n","        _, bt, _,dt = model(input_t,alpha)\n","        latent_xl_t,latent_xq_t = dt\n","        pred_dlt, pred_dqt = bt\n","        err_t_domain = criterion(torch.cat((pred_dlt,pred_dqt)), domain_label_t.to(device))\n","\n","        err = (err_s_label + err_s_label_s) + (err_s_domain + err_t_domain)\n","        err.backward()\n","        optimizer.step()\n","\n","        i += 1\n","        losses_save,accuracy_save,f1_save = test_epoch(epoch,100,source_train,source_test,\\\n","                                                      target_train,target_test,model,criterion)\n","    losses = np.vstack((losses,losses_save))\n","    accuracies=np.vstack((accuracies,accuracy_save))\n","    f1s=np.vstack((f1s,f1_save))\n","\n","    if(epoch % 10 == 0):\n","        print('epoch:{},[{}/{}],s_label:{:.3f},s_label_s:{:.3f},s_domain:{:.3f},t_domain:{:.3f},time{}'.\n","                format(epoch, i, len_dataloader, float(err_s_label), float(err_s_label_s), float(err_s_domain),\n","                        float(err_t_domain), time.time() - start_time))\n","\n","# model_dict = model.state_dict()\n","# exp_name = 'forward'\n","# torch.save(model_dict, \"../../save/model/DA-LS-{}-{}\".format(exp_name,rngseed))\n","\n","# # Save at the end of training\n","# losses2save = {\"Losses\": losses, \"Accuracy\": accuracies, \"F1\": f1s}\n","# with open('../../save/losses/loss-{}-{}'.format(exp_name,rngseed), 'wb') as handle:\n","#     pickle.dump(losses2save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Source Train set: Average loss: 1.3865, 1.6089\n","Source test set: Average loss: 1.3861, 1.6093\n","Target train set: Average loss: 1.3865, 1.6089\n","Target test set: Average loss: 1.3862, 1.6093\n","Source Train set: Average loss: 1.3865, 1.6089\n","Source test set: Average loss: 1.3861, 1.6091\n","Target train set: Average loss: 1.3864, 1.6088\n","Target test set: Average loss: 1.3861, 1.6091\n","Source Train set: Average loss: 1.3865, 1.6089\n","Source test set: Average loss: 1.3861, 1.6092\n","Target train set: Average loss: 1.3864, 1.6089\n","Target test set: Average loss: 1.3861, 1.6092\n","Source Train set: Average loss: 1.3865, 1.6089\n","Source test set: Average loss: 1.3861, 1.6092\n","Target train set: Average loss: 1.3864, 1.6089\n","Target test set: Average loss: 1.3861, 1.6092\n","Source Train set: Average loss: 1.3865, 1.6090\n","Source test set: Average loss: 1.3861, 1.6093\n","Target train set: Average loss: 1.3864, 1.6089\n","Target test set: Average loss: 1.3861, 1.6092\n","Source Train set: Average loss: 1.3865, 1.6090\n","Source test set: Average loss: 1.3861, 1.6093\n","Target train set: Average loss: 1.3864, 1.6089\n","Target test set: Average loss: 1.3861, 1.6093\n","Source Train set: Average loss: 1.3865, 1.6090\n","Source test set: Average loss: 1.3862, 1.6093\n","Target train set: Average loss: 1.3864, 1.6090\n","Target test set: Average loss: 1.3861, 1.6093\n","Source Train set: Average loss: 1.3865, 1.6091\n","Source test set: Average loss: 1.3861, 1.6094\n","Target train set: Average loss: 1.3864, 1.6090\n","Target test set: Average loss: 1.3861, 1.6094\n","Source Train set: Average loss: 1.3865, 1.6091\n","Source test set: Average loss: 1.3861, 1.6094\n","Target train set: Average loss: 1.3864, 1.6090\n","Target test set: Average loss: 1.3861, 1.6094\n","Source Train set: Average loss: 1.3865, 1.6091\n","Source test set: Average loss: 1.3861, 1.6094\n","Target train set: Average loss: 1.3864, 1.6091\n","Target test set: Average loss: 1.3861, 1.6094\n","Source Train set: Average loss: 1.3865, 1.6091\n","Source test set: Average loss: 1.3861, 1.6095\n","Target train set: Average loss: 1.3864, 1.6091\n","Target test set: Average loss: 1.3861, 1.6094\n","Source Train set: Average loss: 1.3865, 1.6092\n","Source test set: Average loss: 1.3861, 1.6095\n","Target train set: Average loss: 1.3864, 1.6091\n","Target test set: Average loss: 1.3861, 1.6095\n","Source Train set: Average loss: 1.3864, 1.6092\n","Source test set: Average loss: 1.3861, 1.6095\n","Target train set: Average loss: 1.3864, 1.6091\n","Target test set: Average loss: 1.3861, 1.6095\n","Source Train set: Average loss: 1.3864, 1.6092\n","Source test set: Average loss: 1.3861, 1.6095\n","Target train set: Average loss: 1.3864, 1.6091\n","Target test set: Average loss: 1.3861, 1.6094\n","Source Train set: Average loss: 1.3864, 1.6092\n","Source test set: Average loss: 1.3861, 1.6094\n","Target train set: Average loss: 1.3864, 1.6091\n","Target test set: Average loss: 1.3861, 1.6094\n","Source Train set: Average loss: 1.3864, 1.6091\n","Source test set: Average loss: 1.3861, 1.6094\n","Target train set: Average loss: 1.3864, 1.6091\n","Target test set: Average loss: 1.3861, 1.6094\n","Source Train set: Average loss: 1.3864, 1.6091\n","Source test set: Average loss: 1.3861, 1.6093\n","Target train set: Average loss: 1.3864, 1.6090\n","Target test set: Average loss: 1.3861, 1.6093\n","Source Train set: Average loss: 1.3864, 1.6090\n","Source test set: Average loss: 1.3861, 1.6092\n","Target train set: Average loss: 1.3864, 1.6090\n","Target test set: Average loss: 1.3861, 1.6092\n","Source Train set: Average loss: 1.3864, 1.6090\n","Source test set: Average loss: 1.3861, 1.6091\n","Target train set: Average loss: 1.3864, 1.6089\n","Target test set: Average loss: 1.3861, 1.6091\n","Source Train set: Average loss: 1.3864, 1.6089\n","Source test set: Average loss: 1.3861, 1.6091\n","Target train set: Average loss: 1.3864, 1.6089\n","Target test set: Average loss: 1.3861, 1.6091\n","Source Train set: Average loss: 1.3864, 1.6089\n","Source test set: Average loss: 1.3860, 1.6090\n","Target train set: Average loss: 1.3864, 1.6088\n","Target test set: Average loss: 1.3861, 1.6090\n","Source Train set: Average loss: 1.3864, 1.6088\n","Source test set: Average loss: 1.3860, 1.6090\n","Target train set: Average loss: 1.3864, 1.6088\n","Target test set: Average loss: 1.3861, 1.6090\n","Source Train set: Average loss: 1.3864, 1.6087\n","Source test set: Average loss: 1.3860, 1.6089\n","Target train set: Average loss: 1.3864, 1.6087\n","Target test set: Average loss: 1.3861, 1.6089\n","Source Train set: Average loss: 1.3864, 1.6087\n","Source test set: Average loss: 1.3860, 1.6089\n","Target train set: Average loss: 1.3864, 1.6086\n","Target test set: Average loss: 1.3861, 1.6089\n","Source Train set: Average loss: 1.3864, 1.6086\n","Source test set: Average loss: 1.3860, 1.6088\n","Target train set: Average loss: 1.3864, 1.6086\n","Target test set: Average loss: 1.3861, 1.6088\n","epoch:0,[24/24],s_label:1.383,s_label_s:1.607,s_domain:0.691,t_domain:0.693,time0.39280033111572266\n","epoch:10,[24/24],s_label:0.988,s_label_s:1.342,s_domain:0.314,t_domain:0.321,time0.42960691452026367\n","epoch:20,[24/24],s_label:0.803,s_label_s:1.237,s_domain:0.314,t_domain:0.314,time0.39391231536865234\n","epoch:30,[24/24],s_label:0.810,s_label_s:1.238,s_domain:0.313,t_domain:0.314,time0.3838460445404053\n","epoch:40,[24/24],s_label:0.746,s_label_s:1.054,s_domain:0.315,t_domain:0.321,time0.3831453323364258\n","epoch:50,[24/24],s_label:0.744,s_label_s:1.036,s_domain:0.318,t_domain:0.322,time0.3902711868286133\n","epoch:60,[24/24],s_label:0.744,s_label_s:1.101,s_domain:0.313,t_domain:0.510,time0.38970446586608887\n","epoch:70,[24/24],s_label:0.744,s_label_s:1.028,s_domain:0.313,t_domain:0.414,time0.40963315963745117\n","epoch:80,[24/24],s_label:0.746,s_label_s:0.959,s_domain:0.317,t_domain:0.376,time0.37927865982055664\n","epoch:90,[24/24],s_label:0.751,s_label_s:1.015,s_domain:0.341,t_domain:0.340,time0.38584089279174805\n","Source Train set: Average loss: 0.7514, 1.0226\n","Source test set: Average loss: 0.7454, 1.0533\n","Target train set: Average loss: 1.4799, 1.7332\n","Target test set: Average loss: 1.4912, 1.7463\n","Source Train set: Average loss: 0.7499, 1.0495\n","Source test set: Average loss: 0.7450, 1.0940\n","Target train set: Average loss: 1.4761, 1.7429\n","Target test set: Average loss: 1.4851, 1.7634\n","Source Train set: Average loss: 0.7486, 1.0674\n","Source test set: Average loss: 0.7442, 1.1113\n","Target train set: Average loss: 1.4673, 1.7470\n","Target test set: Average loss: 1.4719, 1.7700\n","Source Train set: Average loss: 0.7477, 1.0711\n","Source test set: Average loss: 0.7445, 1.1125\n","Target train set: Average loss: 1.4544, 1.7482\n","Target test set: Average loss: 1.4580, 1.7745\n","Source Train set: Average loss: 0.7470, 1.0709\n","Source test set: Average loss: 0.7444, 1.1059\n","Target train set: Average loss: 1.4380, 1.7478\n","Target test set: Average loss: 1.4381, 1.7748\n","Source Train set: Average loss: 0.7508, 1.0680\n","Source test set: Average loss: 0.7448, 1.0873\n","Target train set: Average loss: 1.4215, 1.7433\n","Target test set: Average loss: 1.4210, 1.7715\n","Source Train set: Average loss: 0.7687, 1.1143\n","Source test set: Average loss: 0.7613, 1.1421\n","Target train set: Average loss: 1.4073, 1.7046\n","Target test set: Average loss: 1.4072, 1.7372\n","Source Train set: Average loss: 0.7959, 1.2697\n","Source test set: Average loss: 0.7847, 1.2719\n","Target train set: Average loss: 1.3969, 1.6658\n","Target test set: Average loss: 1.3962, 1.6879\n","Source Train set: Average loss: 0.7869, 1.3392\n","Source test set: Average loss: 0.7734, 1.3377\n","Target train set: Average loss: 1.3909, 1.6937\n","Target test set: Average loss: 1.3899, 1.6814\n","Source Train set: Average loss: 0.7638, 1.2212\n","Source test set: Average loss: 0.7495, 1.2216\n","Target train set: Average loss: 1.3882, 1.6752\n","Target test set: Average loss: 1.3875, 1.6726\n","Source Train set: Average loss: 0.7621, 1.1556\n","Source test set: Average loss: 0.7478, 1.1407\n","Target train set: Average loss: 1.3872, 1.6726\n","Target test set: Average loss: 1.3868, 1.7005\n","Source Train set: Average loss: 0.7603, 1.1964\n","Source test set: Average loss: 0.7496, 1.2213\n","Target train set: Average loss: 1.3868, 1.7328\n","Target test set: Average loss: 1.3865, 1.7576\n","Source Train set: Average loss: 0.7617, 1.2172\n","Source test set: Average loss: 0.7545, 1.2540\n","Target train set: Average loss: 1.3867, 1.7542\n","Target test set: Average loss: 1.3865, 1.7829\n","Source Train set: Average loss: 0.7637, 1.2360\n","Source test set: Average loss: 0.7543, 1.2693\n","Target train set: Average loss: 1.3867, 1.7533\n","Target test set: Average loss: 1.3865, 1.7804\n","Source Train set: Average loss: 0.7640, 1.2416\n","Source test set: Average loss: 0.7564, 1.2789\n","Target train set: Average loss: 1.3867, 1.7524\n","Target test set: Average loss: 1.3866, 1.7795\n","Source Train set: Average loss: 0.7632, 1.2441\n","Source test set: Average loss: 0.7545, 1.2842\n","Target train set: Average loss: 1.3867, 1.7521\n","Target test set: Average loss: 1.3867, 1.7788\n","Source Train set: Average loss: 0.7632, 1.2496\n","Source test set: Average loss: 0.7567, 1.2899\n","Target train set: Average loss: 1.3869, 1.7519\n","Target test set: Average loss: 1.3868, 1.7785\n","Source Train set: Average loss: 0.7611, 1.2422\n","Source test set: Average loss: 0.7533, 1.2882\n","Target train set: Average loss: 1.3871, 1.7519\n","Target test set: Average loss: 1.3870, 1.7785\n","Source Train set: Average loss: 0.7607, 1.2365\n","Source test set: Average loss: 0.7537, 1.2881\n","Target train set: Average loss: 1.3875, 1.7520\n","Target test set: Average loss: 1.3874, 1.7784\n","Source Train set: Average loss: 0.7611, 1.2310\n","Source test set: Average loss: 0.7567, 1.2868\n","Target train set: Average loss: 1.3882, 1.7520\n","Target test set: Average loss: 1.3880, 1.7784\n","Source Train set: Average loss: 0.7612, 1.2213\n","Source test set: Average loss: 0.7580, 1.2760\n","Target train set: Average loss: 1.3901, 1.7520\n","Target test set: Average loss: 1.3900, 1.7784\n","Source Train set: Average loss: 0.7661, 1.2187\n","Source test set: Average loss: 0.7602, 1.2745\n","Target train set: Average loss: 1.3935, 1.7521\n","Target test set: Average loss: 1.3940, 1.7784\n","Source Train set: Average loss: 0.7661, 1.2193\n","Source test set: Average loss: 0.7621, 1.2715\n","Target train set: Average loss: 1.3972, 1.7520\n","Target test set: Average loss: 1.3982, 1.7784\n","Source Train set: Average loss: 0.7738, 1.2342\n","Source test set: Average loss: 0.7695, 1.2848\n","Target train set: Average loss: 1.4036, 1.7519\n","Target test set: Average loss: 1.4051, 1.7785\n","epoch:100,[24/24],s_label:0.744,s_label_s:1.236,s_domain:0.964,t_domain:0.313,time0.4014601707458496\n","epoch:110,[24/24],s_label:0.796,s_label_s:1.229,s_domain:1.165,t_domain:0.695,time0.37981414794921875\n","epoch:120,[24/24],s_label:0.768,s_label_s:1.066,s_domain:0.450,t_domain:0.746,time0.3856017589569092\n","epoch:130,[24/24],s_label:0.751,s_label_s:0.982,s_domain:0.622,t_domain:0.724,time0.3894336223602295\n","epoch:140,[24/24],s_label:0.744,s_label_s:1.025,s_domain:0.693,t_domain:0.693,time0.4028899669647217\n","epoch:150,[24/24],s_label:0.745,s_label_s:1.037,s_domain:0.693,t_domain:0.693,time0.46927404403686523\n","epoch:160,[24/24],s_label:0.744,s_label_s:0.988,s_domain:0.693,t_domain:0.693,time0.41046857833862305\n","epoch:170,[24/24],s_label:0.745,s_label_s:0.981,s_domain:0.314,t_domain:0.707,time0.4048175811767578\n","epoch:180,[24/24],s_label:0.896,s_label_s:1.158,s_domain:0.766,t_domain:0.386,time0.3923003673553467\n","epoch:190,[24/24],s_label:0.783,s_label_s:1.083,s_domain:0.612,t_domain:0.826,time0.4022691249847412\n","Source Train set: Average loss: 0.8002, 1.0830\n","Source test set: Average loss: 0.7916, 1.1029\n","Target train set: Average loss: 1.3893, 1.6822\n","Target test set: Average loss: 1.4172, 1.6722\n","Source Train set: Average loss: 0.7969, 1.0834\n","Source test set: Average loss: 0.7881, 1.1032\n","Target train set: Average loss: 1.3885, 1.6821\n","Target test set: Average loss: 1.4181, 1.6722\n","Source Train set: Average loss: 0.7934, 1.0810\n","Source test set: Average loss: 0.7846, 1.1013\n","Target train set: Average loss: 1.3874, 1.6807\n","Target test set: Average loss: 1.4151, 1.6706\n","Source Train set: Average loss: 0.7886, 1.0742\n","Source test set: Average loss: 0.7796, 1.0965\n","Target train set: Average loss: 1.3866, 1.6784\n","Target test set: Average loss: 1.4126, 1.6702\n","Source Train set: Average loss: 0.7825, 1.0628\n","Source test set: Average loss: 0.7740, 1.0845\n","Target train set: Average loss: 1.3866, 1.6761\n","Target test set: Average loss: 1.4110, 1.6687\n","Source Train set: Average loss: 0.7765, 1.0535\n","Source test set: Average loss: 0.7690, 1.0761\n","Target train set: Average loss: 1.3868, 1.6734\n","Target test set: Average loss: 1.4070, 1.6685\n","Source Train set: Average loss: 0.7712, 1.0470\n","Source test set: Average loss: 0.7656, 1.0686\n","Target train set: Average loss: 1.3867, 1.6724\n","Target test set: Average loss: 1.4068, 1.6681\n","Source Train set: Average loss: 0.7670, 1.0426\n","Source test set: Average loss: 0.7629, 1.0663\n","Target train set: Average loss: 1.3864, 1.6704\n","Target test set: Average loss: 1.4053, 1.6658\n","Source Train set: Average loss: 0.7641, 1.0395\n","Source test set: Average loss: 0.7608, 1.0651\n","Target train set: Average loss: 1.3855, 1.6712\n","Target test set: Average loss: 1.4036, 1.6647\n","Source Train set: Average loss: 0.7623, 1.0364\n","Source test set: Average loss: 0.7595, 1.0634\n","Target train set: Average loss: 1.3842, 1.6719\n","Target test set: Average loss: 1.4003, 1.6628\n","Source Train set: Average loss: 0.7608, 1.0349\n","Source test set: Average loss: 0.7584, 1.0612\n","Target train set: Average loss: 1.3827, 1.6696\n","Target test set: Average loss: 1.3999, 1.6596\n","Source Train set: Average loss: 0.7600, 1.0330\n","Source test set: Average loss: 0.7583, 1.0642\n","Target train set: Average loss: 1.3814, 1.6673\n","Target test set: Average loss: 1.3983, 1.6535\n","Source Train set: Average loss: 0.7595, 1.0315\n","Source test set: Average loss: 0.7574, 1.0606\n","Target train set: Average loss: 1.3804, 1.6711\n","Target test set: Average loss: 1.3961, 1.6584\n","Source Train set: Average loss: 0.7591, 1.0313\n","Source test set: Average loss: 0.7575, 1.0609\n","Target train set: Average loss: 1.3795, 1.6740\n","Target test set: Average loss: 1.3968, 1.6592\n","Source Train set: Average loss: 0.7587, 1.0320\n","Source test set: Average loss: 0.7572, 1.0645\n","Target train set: Average loss: 1.3787, 1.6792\n","Target test set: Average loss: 1.3954, 1.6633\n","Source Train set: Average loss: 0.7585, 1.0346\n","Source test set: Average loss: 0.7569, 1.0672\n","Target train set: Average loss: 1.3781, 1.6855\n","Target test set: Average loss: 1.3943, 1.6709\n","Source Train set: Average loss: 0.7584, 1.0381\n","Source test set: Average loss: 0.7568, 1.0729\n","Target train set: Average loss: 1.3777, 1.6901\n","Target test set: Average loss: 1.3935, 1.6736\n","Source Train set: Average loss: 0.7583, 1.0382\n","Source test set: Average loss: 0.7562, 1.0723\n","Target train set: Average loss: 1.3776, 1.6902\n","Target test set: Average loss: 1.3933, 1.6747\n","Source Train set: Average loss: 0.7582, 1.0368\n","Source test set: Average loss: 0.7566, 1.0702\n","Target train set: Average loss: 1.3778, 1.6888\n","Target test set: Average loss: 1.3951, 1.6739\n","Source Train set: Average loss: 0.7582, 1.0324\n","Source test set: Average loss: 0.7569, 1.0648\n","Target train set: Average loss: 1.3783, 1.6830\n","Target test set: Average loss: 1.3954, 1.6664\n","Source Train set: Average loss: 0.7583, 1.0296\n","Source test set: Average loss: 0.7561, 1.0630\n","Target train set: Average loss: 1.3789, 1.6751\n","Target test set: Average loss: 1.3958, 1.6607\n","Source Train set: Average loss: 0.7585, 1.0297\n","Source test set: Average loss: 0.7562, 1.0577\n","Target train set: Average loss: 1.3795, 1.6678\n","Target test set: Average loss: 1.3981, 1.6561\n","Source Train set: Average loss: 0.7591, 1.0312\n","Source test set: Average loss: 0.7569, 1.0608\n","Target train set: Average loss: 1.3805, 1.6626\n","Target test set: Average loss: 1.3997, 1.6525\n","Source Train set: Average loss: 0.7599, 1.0337\n","Source test set: Average loss: 0.7577, 1.0643\n","Target train set: Average loss: 1.3814, 1.6567\n","Target test set: Average loss: 1.4010, 1.6525\n","epoch:200,[24/24],s_label:0.768,s_label_s:1.014,s_domain:0.608,t_domain:0.740,time0.41044163703918457\n","epoch:210,[24/24],s_label:0.754,s_label_s:1.010,s_domain:0.606,t_domain:0.731,time0.3818166255950928\n","epoch:220,[24/24],s_label:0.790,s_label_s:1.047,s_domain:0.448,t_domain:0.410,time0.3932991027832031\n","epoch:230,[24/24],s_label:0.784,s_label_s:1.139,s_domain:0.378,t_domain:0.385,time0.44570350646972656\n","epoch:240,[24/24],s_label:0.818,s_label_s:1.095,s_domain:0.558,t_domain:0.445,time0.3921020030975342\n","epoch:250,[24/24],s_label:0.779,s_label_s:1.137,s_domain:0.399,t_domain:0.453,time0.4088907241821289\n","epoch:260,[24/24],s_label:0.797,s_label_s:1.088,s_domain:0.449,t_domain:0.395,time0.4127321243286133\n","epoch:270,[24/24],s_label:0.813,s_label_s:1.061,s_domain:0.377,t_domain:0.404,time0.3976860046386719\n","epoch:280,[24/24],s_label:0.834,s_label_s:1.100,s_domain:0.438,t_domain:0.374,time0.43827176094055176\n","epoch:290,[24/24],s_label:0.802,s_label_s:1.090,s_domain:0.377,t_domain:0.359,time0.39699459075927734\n","Source Train set: Average loss: 0.8448, 1.1155\n","Source test set: Average loss: 0.8303, 1.1392\n","Target train set: Average loss: 1.3702, 1.6453\n","Target test set: Average loss: 1.3919, 1.6614\n","Source Train set: Average loss: 0.8438, 1.1168\n","Source test set: Average loss: 0.8291, 1.1402\n","Target train set: Average loss: 1.3705, 1.6464\n","Target test set: Average loss: 1.3912, 1.6644\n","Source Train set: Average loss: 0.8429, 1.1191\n","Source test set: Average loss: 0.8280, 1.1439\n","Target train set: Average loss: 1.3708, 1.6475\n","Target test set: Average loss: 1.3909, 1.6661\n","Source Train set: Average loss: 0.8421, 1.1200\n","Source test set: Average loss: 0.8267, 1.1451\n","Target train set: Average loss: 1.3712, 1.6478\n","Target test set: Average loss: 1.3916, 1.6667\n","Source Train set: Average loss: 0.8412, 1.1199\n","Source test set: Average loss: 0.8260, 1.1430\n","Target train set: Average loss: 1.3713, 1.6477\n","Target test set: Average loss: 1.3902, 1.6664\n","Source Train set: Average loss: 0.8398, 1.1189\n","Source test set: Average loss: 0.8253, 1.1405\n","Target train set: Average loss: 1.3715, 1.6475\n","Target test set: Average loss: 1.3921, 1.6655\n","Source Train set: Average loss: 0.8389, 1.1162\n","Source test set: Average loss: 0.8237, 1.1355\n","Target train set: Average loss: 1.3717, 1.6486\n","Target test set: Average loss: 1.3923, 1.6653\n","Source Train set: Average loss: 0.8382, 1.1138\n","Source test set: Average loss: 0.8222, 1.1335\n","Target train set: Average loss: 1.3718, 1.6491\n","Target test set: Average loss: 1.3921, 1.6661\n","Source Train set: Average loss: 0.8372, 1.1136\n","Source test set: Average loss: 0.8212, 1.1333\n","Target train set: Average loss: 1.3720, 1.6500\n","Target test set: Average loss: 1.3926, 1.6643\n","Source Train set: Average loss: 0.8364, 1.1143\n","Source test set: Average loss: 0.8209, 1.1359\n","Target train set: Average loss: 1.3723, 1.6507\n","Target test set: Average loss: 1.3938, 1.6639\n","Source Train set: Average loss: 0.8354, 1.1158\n","Source test set: Average loss: 0.8201, 1.1352\n","Target train set: Average loss: 1.3723, 1.6516\n","Target test set: Average loss: 1.3930, 1.6642\n","Source Train set: Average loss: 0.8341, 1.1170\n","Source test set: Average loss: 0.8196, 1.1365\n","Target train set: Average loss: 1.3723, 1.6518\n","Target test set: Average loss: 1.3935, 1.6633\n","Source Train set: Average loss: 0.8327, 1.1181\n","Source test set: Average loss: 0.8183, 1.1389\n","Target train set: Average loss: 1.3721, 1.6519\n","Target test set: Average loss: 1.3934, 1.6612\n","Source Train set: Average loss: 0.8309, 1.1168\n","Source test set: Average loss: 0.8182, 1.1385\n","Target train set: Average loss: 1.3717, 1.6518\n","Target test set: Average loss: 1.3941, 1.6613\n","Source Train set: Average loss: 0.8293, 1.1139\n","Source test set: Average loss: 0.8174, 1.1381\n","Target train set: Average loss: 1.3714, 1.6512\n","Target test set: Average loss: 1.3923, 1.6610\n","Source Train set: Average loss: 0.8276, 1.1099\n","Source test set: Average loss: 0.8168, 1.1352\n","Target train set: Average loss: 1.3709, 1.6509\n","Target test set: Average loss: 1.3918, 1.6609\n","Source Train set: Average loss: 0.8260, 1.1066\n","Source test set: Average loss: 0.8154, 1.1356\n","Target train set: Average loss: 1.3707, 1.6493\n","Target test set: Average loss: 1.3932, 1.6605\n","Source Train set: Average loss: 0.8246, 1.1050\n","Source test set: Average loss: 0.8145, 1.1389\n","Target train set: Average loss: 1.3707, 1.6477\n","Target test set: Average loss: 1.3931, 1.6573\n","Source Train set: Average loss: 0.8231, 1.1053\n","Source test set: Average loss: 0.8130, 1.1422\n","Target train set: Average loss: 1.3708, 1.6456\n","Target test set: Average loss: 1.3926, 1.6567\n","Source Train set: Average loss: 0.8215, 1.1073\n","Source test set: Average loss: 0.8114, 1.1486\n","Target train set: Average loss: 1.3709, 1.6444\n","Target test set: Average loss: 1.3927, 1.6551\n","Source Train set: Average loss: 0.8196, 1.1120\n","Source test set: Average loss: 0.8105, 1.1559\n","Target train set: Average loss: 1.3708, 1.6434\n","Target test set: Average loss: 1.3922, 1.6530\n","Source Train set: Average loss: 0.8180, 1.1178\n","Source test set: Average loss: 0.8091, 1.1636\n","Target train set: Average loss: 1.3706, 1.6428\n","Target test set: Average loss: 1.3924, 1.6546\n","Source Train set: Average loss: 0.8168, 1.1204\n","Source test set: Average loss: 0.8087, 1.1666\n","Target train set: Average loss: 1.3706, 1.6436\n","Target test set: Average loss: 1.3928, 1.6530\n","Source Train set: Average loss: 0.8158, 1.1168\n","Source test set: Average loss: 0.8083, 1.1602\n","Target train set: Average loss: 1.3707, 1.6450\n","Target test set: Average loss: 1.3945, 1.6551\n","epoch:300,[24/24],s_label:0.803,s_label_s:1.155,s_domain:0.401,t_domain:0.381,time0.40755343437194824\n","epoch:310,[24/24],s_label:0.799,s_label_s:1.102,s_domain:0.377,t_domain:0.352,time0.40623044967651367\n","epoch:320,[24/24],s_label:0.825,s_label_s:1.147,s_domain:0.328,t_domain:0.341,time0.3940703868865967\n","epoch:330,[24/24],s_label:0.790,s_label_s:1.123,s_domain:0.342,t_domain:0.353,time0.39077329635620117\n","epoch:340,[24/24],s_label:0.782,s_label_s:1.183,s_domain:0.323,t_domain:0.335,time0.40180134773254395\n","epoch:350,[24/24],s_label:0.820,s_label_s:1.073,s_domain:0.324,t_domain:0.338,time0.3904144763946533\n","epoch:360,[24/24],s_label:0.809,s_label_s:1.164,s_domain:0.338,t_domain:0.336,time0.39754748344421387\n","epoch:370,[24/24],s_label:0.831,s_label_s:1.063,s_domain:0.349,t_domain:0.335,time0.3953111171722412\n","epoch:380,[24/24],s_label:0.851,s_label_s:1.198,s_domain:0.387,t_domain:0.359,time0.46152591705322266\n","epoch:390,[24/24],s_label:0.798,s_label_s:1.224,s_domain:0.321,t_domain:0.334,time0.3912644386291504\n","Source Train set: Average loss: 0.8340, 1.1600\n","Source test set: Average loss: 0.8166, 1.1790\n","Target train set: Average loss: 1.3718, 1.6659\n","Target test set: Average loss: 1.3851, 1.6863\n","Source Train set: Average loss: 0.8337, 1.1551\n","Source test set: Average loss: 0.8156, 1.1743\n","Target train set: Average loss: 1.3717, 1.6660\n","Target test set: Average loss: 1.3834, 1.6867\n","Source Train set: Average loss: 0.8334, 1.1499\n","Source test set: Average loss: 0.8153, 1.1691\n","Target train set: Average loss: 1.3716, 1.6661\n","Target test set: Average loss: 1.3835, 1.6876\n","Source Train set: Average loss: 0.8331, 1.1451\n","Source test set: Average loss: 0.8145, 1.1634\n","Target train set: Average loss: 1.3715, 1.6662\n","Target test set: Average loss: 1.3834, 1.6876\n","Source Train set: Average loss: 0.8323, 1.1420\n","Source test set: Average loss: 0.8159, 1.1614\n","Target train set: Average loss: 1.3713, 1.6665\n","Target test set: Average loss: 1.3845, 1.6877\n","Source Train set: Average loss: 0.8315, 1.1399\n","Source test set: Average loss: 0.8139, 1.1596\n","Target train set: Average loss: 1.3710, 1.6668\n","Target test set: Average loss: 1.3832, 1.6883\n","Source Train set: Average loss: 0.8299, 1.1389\n","Source test set: Average loss: 0.8140, 1.1574\n","Target train set: Average loss: 1.3706, 1.6672\n","Target test set: Average loss: 1.3825, 1.6890\n","Source Train set: Average loss: 0.8283, 1.1402\n","Source test set: Average loss: 0.8123, 1.1588\n","Target train set: Average loss: 1.3703, 1.6675\n","Target test set: Average loss: 1.3822, 1.6890\n","Source Train set: Average loss: 0.8273, 1.1409\n","Source test set: Average loss: 0.8123, 1.1575\n","Target train set: Average loss: 1.3699, 1.6678\n","Target test set: Average loss: 1.3825, 1.6891\n","Source Train set: Average loss: 0.8270, 1.1416\n","Source test set: Average loss: 0.8128, 1.1585\n","Target train set: Average loss: 1.3697, 1.6681\n","Target test set: Average loss: 1.3820, 1.6893\n","Source Train set: Average loss: 0.8270, 1.1410\n","Source test set: Average loss: 0.8123, 1.1569\n","Target train set: Average loss: 1.3694, 1.6682\n","Target test set: Average loss: 1.3816, 1.6896\n","Source Train set: Average loss: 0.8273, 1.1387\n","Source test set: Average loss: 0.8128, 1.1554\n","Target train set: Average loss: 1.3692, 1.6683\n","Target test set: Average loss: 1.3818, 1.6903\n","Source Train set: Average loss: 0.8277, 1.1368\n","Source test set: Average loss: 0.8145, 1.1515\n","Target train set: Average loss: 1.3690, 1.6683\n","Target test set: Average loss: 1.3811, 1.6901\n","Source Train set: Average loss: 0.8280, 1.1358\n","Source test set: Average loss: 0.8140, 1.1496\n","Target train set: Average loss: 1.3688, 1.6685\n","Target test set: Average loss: 1.3804, 1.6899\n","Source Train set: Average loss: 0.8282, 1.1361\n","Source test set: Average loss: 0.8148, 1.1495\n","Target train set: Average loss: 1.3687, 1.6688\n","Target test set: Average loss: 1.3802, 1.6904\n","Source Train set: Average loss: 0.8284, 1.1373\n","Source test set: Average loss: 0.8163, 1.1497\n","Target train set: Average loss: 1.3686, 1.6691\n","Target test set: Average loss: 1.3801, 1.6908\n","Source Train set: Average loss: 0.8283, 1.1390\n","Source test set: Average loss: 0.8166, 1.1517\n","Target train set: Average loss: 1.3685, 1.6693\n","Target test set: Average loss: 1.3801, 1.6905\n","Source Train set: Average loss: 0.8280, 1.1409\n","Source test set: Average loss: 0.8156, 1.1529\n","Target train set: Average loss: 1.3685, 1.6695\n","Target test set: Average loss: 1.3808, 1.6908\n","Source Train set: Average loss: 0.8276, 1.1423\n","Source test set: Average loss: 0.8150, 1.1535\n","Target train set: Average loss: 1.3685, 1.6697\n","Target test set: Average loss: 1.3814, 1.6912\n","Source Train set: Average loss: 0.8270, 1.1434\n","Source test set: Average loss: 0.8142, 1.1559\n","Target train set: Average loss: 1.3686, 1.6700\n","Target test set: Average loss: 1.3812, 1.6922\n","Source Train set: Average loss: 0.8265, 1.1446\n","Source test set: Average loss: 0.8142, 1.1586\n","Target train set: Average loss: 1.3686, 1.6701\n","Target test set: Average loss: 1.3805, 1.6920\n","Source Train set: Average loss: 0.8260, 1.1436\n","Source test set: Average loss: 0.8129, 1.1589\n","Target train set: Average loss: 1.3686, 1.6702\n","Target test set: Average loss: 1.3810, 1.6925\n","Source Train set: Average loss: 0.8255, 1.1420\n","Source test set: Average loss: 0.8125, 1.1595\n","Target train set: Average loss: 1.3686, 1.6701\n","Target test set: Average loss: 1.3809, 1.6928\n","Source Train set: Average loss: 0.8250, 1.1400\n","Source test set: Average loss: 0.8125, 1.1599\n","Target train set: Average loss: 1.3687, 1.6701\n","Target test set: Average loss: 1.3813, 1.6927\n","epoch:400,[24/24],s_label:0.832,s_label_s:1.133,s_domain:0.395,t_domain:0.329,time0.4706451892852783\n","epoch:410,[24/24],s_label:0.786,s_label_s:1.143,s_domain:0.361,t_domain:0.331,time0.39716267585754395\n","epoch:420,[24/24],s_label:0.811,s_label_s:1.086,s_domain:0.328,t_domain:0.353,time0.38947319984436035\n","epoch:430,[24/24],s_label:0.792,s_label_s:1.122,s_domain:0.315,t_domain:0.344,time0.40548181533813477\n","epoch:440,[24/24],s_label:0.821,s_label_s:1.147,s_domain:0.385,t_domain:0.328,time0.396622896194458\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OjdjlZh8mesh"},"source":["import matplotlib.pyplot as plt\n","\n","fig=plt.figure(figsize=(6,4))\n","losses = np.array(losses)\n","color = ['r','k','b','m']\n","for i in range(4):\n","    plt.plot(losses[:,i*2],c=color[i],ls='--')\n","    plt.plot(losses[:,i*2+1],c=color[i],ls='-')\n","lgd=plt.legend(['Source, train, localization','Source, train, quantification',\\\n","           'Source, test, localization','Source, test, quantification',\\\n","           'Target, train, localization','Target, train, quantification',\\\n","           'Target, test, localization','Target, test, Quantification'],loc=2,bbox_to_anchor=(1, 1),fontsize=10)\n","plt.xlabel('# epoch',fontsize=12)\n","plt.ylabel('Loss',fontsize=12)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Z-3EhBHmhWl"},"source":["accuracies = np.array(accuracies)\n","color = ['r','k','b','m']\n","norm = [390*0.8,390*0.2,390*0.8,390*0.2]\n","for i in range(4):\n","    plt.plot(accuracies[:,i*2]/norm[i],c=color[i],ls='--')\n","    plt.plot(accuracies[:,i*2+1]/norm[i],c=color[i],ls='-')\n","plt.legend(['S, train, L','S, train, Q',\\\n","           'S, test, L','S, test, Q',\\\n","           'T, train, L','T, train, Q',\\\n","           'T, test, L','T, test, Q'])\n","plt.title('Accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MnxeXiSBGWBM"},"source":[""],"execution_count":null,"outputs":[]}]}